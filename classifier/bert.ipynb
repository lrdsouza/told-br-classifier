{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb772f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"TRUE\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8cffe0",
   "metadata": {},
   "source": [
    "### Read NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8198edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../datasets/train.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f4bb6",
   "metadata": {},
   "source": [
    "### Clear Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e5ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clear(text):\n",
    "    text = text.replace('rt @user', '')\n",
    "    text = text.replace('@user', '')\n",
    "    pattern = re.compile('[^a-zA-Z0-9\\sáéíóúàèìòùâêîôûãõçÁÉÍÓÚÀÈÌÒÙÂÊÎÔÛÃÕÇ]')\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = pattern.sub(r' ', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89cd4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.apply(lambda x: Clear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715e9cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Toxic - 0.56% <-> Toxic - 0.44%\n"
     ]
    }
   ],
   "source": [
    "x = Counter(data.label).most_common()\n",
    "m = x[0][1]/(x[0][1]+x[1][1])\n",
    "print('Non-Toxic - {}% <-> Toxic - {}%'.format(round(m, 2), round(1-m, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a20d7d",
   "metadata": {},
   "source": [
    "### Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87adc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(data, test_size=0.1, random_state=0)\n",
    "train = train.reset_index(drop=True)\n",
    "validation = validation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ae232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Toxic - 0.56% <-> Toxic - 0.44%\n"
     ]
    }
   ],
   "source": [
    "x = Counter(train.label).most_common()\n",
    "m = x[0][1]/(x[0][1]+x[1][1])\n",
    "print('Non-Toxic - {}% <-> Toxic - {}%'.format(round(m, 2), round(1-m, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed5b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Toxic - 0.57% <-> Toxic - 0.43%\n"
     ]
    }
   ],
   "source": [
    "x = Counter(validation.label).most_common()\n",
    "m = x[0][1]/(x[0][1]+x[1][1])\n",
    "print('Non-Toxic - {}% <-> Toxic - {}%'.format(round(m, 2), round(1-m, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1cc44",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eee2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train)\n",
    "validation_dataset = Dataset.from_pandas(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d0612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict()\n",
    "datasets['train'] = train_dataset\n",
    "datasets['validation'] = validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310b778",
   "metadata": {},
   "source": [
    "### Tokenize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daf8d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '../model-foundation/finetuned/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89bc7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd0b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenizerFunc(input_):\n",
    "    result = tokenizer(input_['text'], padding=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d581800a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0379a6c49e47f6b1aedd3370f7243b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/15120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c926a22fd484580b150a59f29db04a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokens = datasets.map(TokenizerFunc, batched=True, num_proc=4, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db8d98",
   "metadata": {},
   "source": [
    "### FineTune BERTimbau Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c38fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model-foundation/finetuned/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model-foundation/finetuned/ and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "572b092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/finetune_metadata/checkpoints/'\n",
    "logging_dir = '/finetune_metadata/logs/'\n",
    "    \n",
    "training_args = TrainingArguments(output_dir=output_dir,\n",
    "                                  logging_dir=logging_dir,\n",
    "                                  max_steps=1000,\n",
    "                                  learning_rate=1e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                  adam_beta1 = 0.7,\n",
    "                                  adam_beta2 = 0.999,\n",
    "                                  adam_epsilon = 2e-08,\n",
    "                                  max_grad_norm = 1.5,\n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  logging_steps=50,\n",
    "                                  warmup_steps=100,\n",
    "                                  gradient_accumulation_steps=1,\n",
    "                                  seed=42,\n",
    "                                  metric_for_best_model='eval_loss',\n",
    "                                  logging_strategy='steps',\n",
    "                                  evaluation_strategy='steps',\n",
    "                                  greater_is_better=False,\n",
    "                                  do_train=True,\n",
    "                                  do_eval=True,\n",
    "                                  do_predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f62d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19f2ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMetrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26b1a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=dataset_tokens['train'],\n",
    "                  eval_dataset=dataset_tokens['validation'],\n",
    "                  compute_metrics=ComputeMetrics,\n",
    "                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352355cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 12:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.696700</td>\n",
       "      <td>0.685183</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.200426</td>\n",
       "      <td>0.425339</td>\n",
       "      <td>0.131102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>0.672272</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.605469</td>\n",
       "      <td>0.432357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.641894</td>\n",
       "      <td>0.619643</td>\n",
       "      <td>0.221681</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.126918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.601500</td>\n",
       "      <td>0.554062</td>\n",
       "      <td>0.746429</td>\n",
       "      <td>0.677761</td>\n",
       "      <td>0.740496</td>\n",
       "      <td>0.624826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.545100</td>\n",
       "      <td>0.517344</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.668770</td>\n",
       "      <td>0.769510</td>\n",
       "      <td>0.591353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.492633</td>\n",
       "      <td>0.765476</td>\n",
       "      <td>0.708580</td>\n",
       "      <td>0.754331</td>\n",
       "      <td>0.668061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.591300</td>\n",
       "      <td>0.505627</td>\n",
       "      <td>0.769643</td>\n",
       "      <td>0.737271</td>\n",
       "      <td>0.718254</td>\n",
       "      <td>0.757322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.475800</td>\n",
       "      <td>0.536851</td>\n",
       "      <td>0.761310</td>\n",
       "      <td>0.754139</td>\n",
       "      <td>0.672867</td>\n",
       "      <td>0.857741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>0.493465</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.752093</td>\n",
       "      <td>0.698565</td>\n",
       "      <td>0.814505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.589685</td>\n",
       "      <td>0.745238</td>\n",
       "      <td>0.758192</td>\n",
       "      <td>0.637227</td>\n",
       "      <td>0.935844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.468726</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.740845</td>\n",
       "      <td>0.748222</td>\n",
       "      <td>0.733612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.471738</td>\n",
       "      <td>0.777976</td>\n",
       "      <td>0.742581</td>\n",
       "      <td>0.734973</td>\n",
       "      <td>0.750349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.480323</td>\n",
       "      <td>0.783929</td>\n",
       "      <td>0.768937</td>\n",
       "      <td>0.707260</td>\n",
       "      <td>0.842399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.531200</td>\n",
       "      <td>0.483352</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.767980</td>\n",
       "      <td>0.696145</td>\n",
       "      <td>0.856346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.495817</td>\n",
       "      <td>0.780357</td>\n",
       "      <td>0.764217</td>\n",
       "      <td>0.705189</td>\n",
       "      <td>0.834031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.425300</td>\n",
       "      <td>0.505059</td>\n",
       "      <td>0.777381</td>\n",
       "      <td>0.757772</td>\n",
       "      <td>0.707376</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.719368</td>\n",
       "      <td>0.761506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.569200</td>\n",
       "      <td>0.489699</td>\n",
       "      <td>0.777976</td>\n",
       "      <td>0.745740</td>\n",
       "      <td>0.729333</td>\n",
       "      <td>0.762901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.582700</td>\n",
       "      <td>0.480333</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.747599</td>\n",
       "      <td>0.735493</td>\n",
       "      <td>0.760112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.480543</td>\n",
       "      <td>0.781548</td>\n",
       "      <td>0.751186</td>\n",
       "      <td>0.730871</td>\n",
       "      <td>0.772664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.5496741771697998, metrics={'train_runtime': 724.1919, 'train_samples_per_second': 5.523, 'train_steps_per_second': 1.381, 'total_flos': 324753042298560.0, 'train_loss': 0.5496741771697998, 'epoch': 0.26})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e6b03ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned/tokenizer_config.json',\n",
       " './finetuned/special_tokens_map.json',\n",
       " './finetuned/vocab.txt',\n",
       " './finetuned/added_tokens.json',\n",
       " './finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = './finetuned/'\n",
    "trainer.save_model(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6a067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
